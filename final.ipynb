{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample\n",
    "from sklearn import datasets\n",
    "import NeuralNet as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-Written Digits Dataset\n",
    "\n",
    "**Models Used:**\n",
    "- Neural Networks\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(as_frame=True)\n",
    "dig_df = digits['data']\n",
    "dig_df['class'] = digits['target']\n",
    "dig_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups the combine into folds\n",
    "k = 10\n",
    "dig_class_0 = dig_df.loc[dig_df['class'] == 0].sample(frac=1)\n",
    "dig_class_0['class'] = [[1,0,0,0,0,0,0,0,0,0]] * len(dig_class_0)\n",
    "dg0_split =  np.array_split(dig_class_0,k)\n",
    "dig_class_1 = dig_df.loc[dig_df['class'] == 1].sample(frac=1)\n",
    "dig_class_1['class'] = [[0,1,0,0,0,0,0,0,0,0]] * len(dig_class_1)\n",
    "dg1_split =  np.array_split(dig_class_1,k)\n",
    "dig_class_2 = dig_df.loc[dig_df['class'] == 2].sample(frac=1)\n",
    "dig_class_2['class'] = [[0,0,1,0,0,0,0,0,0,0]] * len(dig_class_2)\n",
    "dg2_split =  np.array_split(dig_class_2,k)\n",
    "dig_class_3 = dig_df.loc[dig_df['class'] == 3].sample(frac=1)\n",
    "dig_class_3['class'] = [[0,0,0,1,0,0,0,0,0,0]] * len(dig_class_3)\n",
    "dg3_split =  np.array_split(dig_class_3,k)\n",
    "dig_class_4 = dig_df.loc[dig_df['class'] == 4].sample(frac=1)\n",
    "dig_class_4['class'] = [[0,0,0,0,1,0,0,0,0,0]] * len(dig_class_4)\n",
    "dg4_split =  np.array_split(dig_class_4,k)\n",
    "dig_class_5 = dig_df.loc[dig_df['class'] == 5].sample(frac=1)\n",
    "dig_class_5['class'] = [[0,0,0,0,0,1,0,0,0,0]] * len(dig_class_5)\n",
    "dg5_split =  np.array_split(dig_class_5,k)\n",
    "dig_class_6 = dig_df.loc[dig_df['class'] == 6].sample(frac=1)\n",
    "dig_class_6['class'] = [[0,0,0,0,0,0,1,0,0,0]] * len(dig_class_6)\n",
    "dg6_split =  np.array_split(dig_class_6,k)\n",
    "dig_class_7 = dig_df.loc[dig_df['class'] == 7].sample(frac=1)\n",
    "dig_class_7['class'] = [[0,0,0,0,0,0,0,1,0,0]] * len(dig_class_7)\n",
    "dg7_split =  np.array_split(dig_class_7,k)\n",
    "dig_class_8 = dig_df.loc[dig_df['class'] == 8].sample(frac=1)\n",
    "dig_class_8['class'] = [[0,0,0,0,0,0,0,0,1,0]] * len(dig_class_8)\n",
    "dg8_split =  np.array_split(dig_class_8,k)\n",
    "dig_class_9 = dig_df.loc[dig_df['class'] == 9].sample(frac=1)\n",
    "dig_class_9['class'] = [[0,0,0,0,0,0,0,0,0,1]] * len(dig_class_9)\n",
    "dg9_split =  np.array_split(dig_class_9,k)\n",
    "dig_vals = [[1,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "dig_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [dg0_split[i],dg1_split[i],dg2_split[i],dg3_split[i],dg4_split[i],dg5_split[i],dg6_split[i],dg7_split[i],dg8_split[i],dg9_split[i]]\n",
    "    dig_fold.append(pd.concat(this_fold))\n",
    "\n",
    "#dig_nn_arc = [[64,64,10],[64,128,10],[64,64,128,10],[64,32,64,10],[64,64,32,64,10],[64,64,128,128,64,10]]\n",
    "dig_nn_arc = [[64,64,10],[64,128,10]]\n",
    "\n",
    "def dig_test(lamb,eps,alpha,batch_size):\n",
    "    dig_res = nn.k_fold(dig_fold,dig_vals,dig_nn_arc,lamb,eps,alpha,batch_size)\n",
    "    arc_dict = defaultdict(list)\n",
    "    print(f'lamb = {lamb} eps = {eps} alpha = {alpha} batch_size = {batch_size}')\n",
    "\n",
    "    for arc,perf in dig_res.items():\n",
    "        avg_acc,avg_f1 = [0,0]\n",
    "        for res in perf:\n",
    "            avg_acc += res[0]\n",
    "            avg_f1 += res[1]\n",
    "        arc_dict['Architecture'].append(arc)\n",
    "        arc_dict['Accuracy'].append(avg_acc/10)\n",
    "        arc_dict['F1'].append(avg_f1/10)\n",
    "\n",
    "    arc_table = pd.DataFrame(arc_dict)\n",
    "    print(arc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.4 eps = 0.01 alpha = 10 batch_size = 50\n",
      "        Architecture  Accuracy        F1\n",
      "0       [64, 64, 10]  0.951643  0.951395\n",
      "1      [64, 128, 10]  0.950545  0.949829\n",
      "2  [64, 64, 128, 10]  0.925966  0.925795\n"
     ]
    }
   ],
   "source": [
    "hyper_params = [[0.4,0.01,10,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.4 eps = 0.01 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.957663  0.957519\n",
      "1  [64, 128, 10]  0.962179  0.961978\n",
      "lamb = 0.4 eps = 0.01 alpha = 10 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.940580  0.941175\n",
      "1  [64, 128, 10]  0.945464  0.946779\n",
      "lamb = 0.2 eps = 0.01 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.930997  0.930357\n",
      "1  [64, 128, 10]  0.948146  0.948122\n",
      "lamb = 0.2 eps = 0.01 alpha = 10 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.946610  0.946156\n",
      "1  [64, 128, 10]  0.957118  0.957053\n"
     ]
    }
   ],
   "source": [
    "hyper_params = [[0.4,0.01,5,50],[0.4,0.01,10,50],[0.2,0.01,5,50],[0.2,0.01,10,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.4 eps = 0.01 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.955459  0.955166\n",
      "1  [64, 128, 10]  0.958234  0.957735\n",
      "lamb = 0.6 eps = 0.01 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.958786  0.959035\n",
      "1  [64, 128, 10]  0.964924  0.964825\n",
      "lamb = 0.4 eps = 0.001 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.964949  0.965058\n",
      "1  [64, 128, 10]  0.967716  0.967639\n",
      "lamb = 0.6 eps = 0.001 alpha = 5 batch_size = 50\n",
      "    Architecture  Accuracy        F1\n",
      "0   [64, 64, 10]  0.964918  0.964842\n",
      "1  [64, 128, 10]  0.968833  0.968655\n"
     ]
    }
   ],
   "source": [
    "hyper_params = [[0.4,0.01,5,50],[0.6,0.01,5,50],[0.4,0.001,5,50],[0.6,0.001,5,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.2 eps = 0.001 alpha = 5 batch_size = 50\n",
      "        Architecture  Accuracy        F1\n",
      "0       [64, 64, 10]  0.965455  0.965219\n",
      "1      [64, 128, 10]  0.972150  0.972106\n",
      "2  [64, 64, 128, 10]  0.962743  0.962414\n",
      "3   [64, 32, 64, 10]  0.946058  0.945727\n",
      "lamb = 0.4 eps = 0.001 alpha = 5 batch_size = 50\n",
      "        Architecture  Accuracy        F1\n",
      "0       [64, 64, 10]  0.957652  0.957369\n",
      "1      [64, 128, 10]  0.972685  0.972667\n",
      "2  [64, 64, 128, 10]  0.959403  0.959354\n",
      "3   [64, 32, 64, 10]  0.945982  0.945710\n"
     ]
    }
   ],
   "source": [
    "#dig_nn_arc = [[64,64,10],[64,128,10],[64,64,128,10],[64,32,64,10],[64,64,32,64,10],[64,64,128,128,64,10]]\n",
    "dig_nn_arc = [[64,64,10],[64,128,10],[64,64,128,10],[64,32,64,10]]\n",
    "hyper_params = [[0.2,0.001,5,50],[0.4,0.001,5,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.2 eps = 0.0001 alpha = 7 batch_size = 50\n",
      "         Architecture  Accuracy        F1\n",
      "0       [64, 128, 10]  0.966064  0.965883\n",
      "1  [64, 128, 128, 10]  0.972193  0.972465\n",
      "lamb = 0.2 eps = 0.0001 alpha = 10 batch_size = 50\n",
      "         Architecture  Accuracy        F1\n",
      "0       [64, 128, 10]  0.966079  0.965760\n",
      "1  [64, 128, 128, 10]  0.961644  0.961582\n"
     ]
    }
   ],
   "source": [
    "#dig_nn_arc = [[64,64,10],[64,128,10],[64,64,128,10],[64,32,64,10],[64,64,32,64,10],[64,64,128,128,64,10]]\n",
    "dig_nn_arc = [[64,128,10],[64,128,128,10]]\n",
    "hyper_params = [[0.2,0.0001,7,50],[0.2,0.0001,10,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.2 eps = 0.0001 alpha = 5 batch_size = 50\n",
      "            Architecture  Accuracy        F1\n",
      "0           [64, 64, 10]  0.959416  0.959145\n",
      "1          [64, 128, 10]  0.969383  0.969181\n",
      "2     [64, 128, 128, 10]  0.972754  0.972740\n",
      "3  [64, 64, 128, 64, 10]  0.963294  0.963047\n",
      "lamb = 0.2 eps = 0.0001 alpha = 7 batch_size = 50\n",
      "            Architecture  Accuracy        F1\n",
      "0           [64, 64, 10]  0.957667  0.957718\n",
      "1          [64, 128, 10]  0.972792  0.972715\n",
      "2     [64, 128, 128, 10]  0.970452  0.970365\n",
      "3  [64, 64, 128, 64, 10]  0.962088  0.961845\n"
     ]
    }
   ],
   "source": [
    "#dig_nn_arc = [[64,64,10],[64,128,10],[64,64,128,10],[64,32,64,10],[64,64,32,64,10],[64,64,128,128,64,10]]\n",
    "dig_nn_arc = [[64,64,10],[64,128,10],[64,128,128,10],[64,64,128,64,10]]\n",
    "hyper_params = [[0.2,0.0001,5,50],[0.2,0.0001,7,50]]\n",
    "for params in hyper_params:\n",
    "    dig_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Dataset\n",
    "\n",
    "**Models Used:**\n",
    "- Naive Bayes\n",
    "- Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Eligibility Prediction Dataset\n",
    "\n",
    "**Models Used:**\n",
    "- Random Forests\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oxford Parkingson's Disease Detection\n",
    "\n",
    "**Models Used:**\n",
    "- Neural Networks\n",
    "- Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
